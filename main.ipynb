{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before start\n",
    "- Initialize the environment running this command (optional): \n",
    " python -m venv .venv\n",
    "- Don't forget to add your OpenAI API key on .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libs to install\n",
    "!pip install langchain\n",
    "!pip install python-dotenv\n",
    "!pip install openai\n",
    "!pip install pypdf\n",
    "!pip install bs4\n",
    "!pip install unstructured[local-inference] -q\n",
    "!pip install selenium\n",
    "!pip install pydantic-settings\n",
    "!pip install chromadb\n",
    "!pip install tiktoken\n",
    "!pip install fastapi nest-asyncio pyngrok uvicorn\n",
    "!pip install rdflib SPARQLWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries & GPT Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import openai\n",
    "import datetime\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import SeleniumURLLoader\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import OpenAI\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from pyngrok import ngrok\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT API settings\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatWithGPT(prompt, model=llm_model):\n",
    "    \"\"\"\n",
    "    This function sends a given message to ChatGPT API and returns its answer\n",
    "        :prompt: is the user prompt\n",
    "        :model: (optional) indicates the GPT model\n",
    "        :return: returns the answer from ChatGPT\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "def getAllData(data_dirpath):\n",
    "    \"\"\"\n",
    "    This function loads all the data located in the given directory\n",
    "        :data_dirpath: Directory path of the folder that contains all files to load\n",
    "        :return: returns all data in pages format\n",
    "    \"\"\"\n",
    "    # Read PDFs\n",
    "    pdf_loader = DirectoryLoader(data_dirpath, glob=\"**/*.pdf\")\n",
    "        \n",
    "    # Read web URLs in .txt\n",
    "    with open(data_dirpath + \"/\" + \"webURLs.txt\") as f:\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        \n",
    "    webpages_loader = SeleniumURLLoader(urls=lines)\n",
    "    \n",
    "    loaders = [pdf_loader, webpages_loader]\n",
    "    documents = []\n",
    "    \n",
    "    for loader in loaders:\n",
    "        documents.extend(loader.load())\n",
    "                    \n",
    "    return documents            \n",
    "    \n",
    "def getChunkText(documents):\n",
    "    \"\"\"\n",
    "    This function splits the pages into chunks by \".\" every 1000 characters or more,\n",
    "    with an overlap of 200 and use the len() function to count them.\n",
    "        :documents: pages data\n",
    "        :return: chunks of the data\n",
    "    \"\"\"\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "    separator = \".\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200,\n",
    "    length_function = len\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n",
    "def chatWithDocs(question, chat_history):\n",
    "    \"\"\"\n",
    "    This function receives an user prompt and return an embedded response from OpenAI, according to the \n",
    "    previously provided data.\n",
    "        :documents: pages data\n",
    "        :return: chunks of the data\n",
    "    \"\"\"\n",
    "    response = chat_with_docs({\"question\": question,\n",
    "                           \"chat_history\": chat_history})\n",
    "    return response[\"answer\"]\n",
    "\n",
    "def connectWithAnzograph(chat_history):\n",
    "    \"\"\"\n",
    "    This function connects and insert chat's data in the Anzograph knowledge graph.\n",
    "    The insertion its created by an ChatGPT prompt (to get the triplets)\n",
    "        :chat_history: chat data\n",
    "        :return: none\n",
    "    \"\"\"\n",
    "    prompt_to_gpt = \"Convert the following text delimited by curl brackets into a SPARQL insert query without any prefix to a graph called routines_exercises: {{{\" + str(chat_history) + \"}}}\"\n",
    "    gpt_response = chatWithGPT(prompt_to_gpt)\n",
    "    \n",
    "    username = \"admin\"\n",
    "    password = \"Passw0rd1\"\n",
    "        \n",
    "    try:\n",
    "        sparql_wrapper = SPARQLWrapper(\"http://localhost:80/\")\n",
    "        \n",
    "        sparql_wrapper.method = 'POST'\n",
    "        sparql_wrapper.setCredentials(username, password)\n",
    "        sparql_wrapper.setQuery(f\"\"\"{gpt_response}\"\"\".encode(\"utf-8\"))\n",
    "        sparql_wrapper.setReturnFormat(JSON)\n",
    "        \n",
    "        results = sparql_wrapper.query().convert()\n",
    "        \n",
    "        if results and \"results\" in results:\n",
    "            print(\"Connected to AnzoGraph successfully!\")\n",
    "        else:\n",
    "            print(\"Failed to connect to AnzoGraph. Check the server URL and credentials.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"An error ocurred: \" + str(e))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = getAllData(\"data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### documents in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2851, which is longer than the specified 1000\n",
      "Created a chunk of size 1488, which is longer than the specified 1000\n",
      "Created a chunk of size 2645, which is longer than the specified 1000\n",
      "Created a chunk of size 2234, which is longer than the specified 1000\n",
      "Created a chunk of size 4316, which is longer than the specified 1000\n",
      "Created a chunk of size 1524, which is longer than the specified 1000\n",
      "Created a chunk of size 2181, which is longer than the specified 1000\n",
      "Created a chunk of size 1079, which is longer than the specified 1000\n",
      "Created a chunk of size 1380, which is longer than the specified 1000\n",
      "Created a chunk of size 1521, which is longer than the specified 1000\n",
      "Created a chunk of size 2118, which is longer than the specified 1000\n",
      "Created a chunk of size 2533, which is longer than the specified 1000\n",
      "Created a chunk of size 1529, which is longer than the specified 1000\n",
      "Created a chunk of size 2744, which is longer than the specified 1000\n",
      "Created a chunk of size 3120, which is longer than the specified 1000\n",
      "Created a chunk of size 1085, which is longer than the specified 1000\n",
      "Created a chunk of size 3024, which is longer than the specified 1000\n",
      "Created a chunk of size 1170, which is longer than the specified 1000\n",
      "Created a chunk of size 1139, which is longer than the specified 1000\n",
      "Created a chunk of size 1827, which is longer than the specified 1000\n",
      "Created a chunk of size 2154, which is longer than the specified 1000\n",
      "Created a chunk of size 1024, which is longer than the specified 1000\n",
      "Created a chunk of size 2258, which is longer than the specified 1000\n",
      "Created a chunk of size 1017, which is longer than the specified 1000\n",
      "Created a chunk of size 1095, which is longer than the specified 1000\n",
      "Created a chunk of size 10739, which is longer than the specified 1000\n",
      "Created a chunk of size 1314, which is longer than the specified 1000\n",
      "Created a chunk of size 1008, which is longer than the specified 1000\n",
      "Created a chunk of size 1191, which is longer than the specified 1000\n",
      "Created a chunk of size 1096, which is longer than the specified 1000\n",
      "Created a chunk of size 1005, which is longer than the specified 1000\n",
      "Created a chunk of size 1096, which is longer than the specified 1000\n",
      "Created a chunk of size 1010, which is longer than the specified 1000\n",
      "Created a chunk of size 2820, which is longer than the specified 1000\n",
      "Created a chunk of size 1649, which is longer than the specified 1000\n",
      "Created a chunk of size 1816, which is longer than the specified 1000\n",
      "Created a chunk of size 1349, which is longer than the specified 1000\n",
      "Created a chunk of size 1443, which is longer than the specified 1000\n",
      "Created a chunk of size 1688, which is longer than the specified 1000\n",
      "Created a chunk of size 2205, which is longer than the specified 1000\n",
      "Created a chunk of size 1029, which is longer than the specified 1000\n",
      "Created a chunk of size 1582, which is longer than the specified 1000\n",
      "Created a chunk of size 6504, which is longer than the specified 1000\n",
      "Created a chunk of size 5245, which is longer than the specified 1000\n",
      "Created a chunk of size 4329, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "chunks = getChunkText(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chunks in output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunks[22])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Langchain - Conversation Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "chat_with_docs = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatWithDocs(\"Hi, I would like to do some exercise. I want to gain legs muscles\", chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatWithDocs(\"que es la maquina de turing\", chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POST request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=['*'],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=['*'],\n",
    "    allow_headers=['*'],\n",
    ")\n",
    "\n",
    "class Prompt(BaseModel):\n",
    "    user_prompt : str\n",
    "\n",
    "@app.post('/chat_with_docs')\n",
    "async def Post_prompt(prompt : Prompt):\n",
    "    return {\"response\" : chatWithDocs(prompt.user_prompt, chat_history)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t=2023-09-15T14:56:28-0400 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=C:\\\\Users\\\\mayor\\\\AppData\\\\Local/ngrok/ngrok.yml legacy_path=C:\\\\Users\\\\mayor\\\\.ngrok2\\\\ngrok.yml\n",
      "INFO:     Started server process [9420]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public URL: https://524b-200-58-68-43.ngrok.io\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ngrok_tunnel = ngrok.connect(8000)\n",
    "    print('Public URL:', ngrok_tunnel.public_url)\n",
    "    nest_asyncio.apply()\n",
    "    uvicorn.run(app, port=8000)\n",
    "except Exception as e:\n",
    "    print(\"An error ocurred: \" + str(e))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
