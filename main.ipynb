{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before start\n",
    "- Initialize the environment running this command: \n",
    " python -m venv .venv\n",
    "- Don't forget to add your API key on .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libs to install\n",
    "!pip install langchain\n",
    "!pip install python-dotenv\n",
    "!pip install openai\n",
    "!pip install pypdf\n",
    "!pip install bs4\n",
    "!pip install unstructured[local-inference] -q\n",
    "!pip install selenium\n",
    "!pip install pydantic-settings\n",
    "!pip install chromadb\n",
    "!pip install tiktoken\n",
    "!pip install fastapi nest-asyncio pyngrok uvicorn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries & GPT Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import openai\n",
    "import datetime\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import SeleniumURLLoader\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import OpenAI\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from pyngrok import ngrok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT API settings\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatWithGPT(prompt, model=llm_model):\n",
    "    \"\"\"\n",
    "    chatWithGPT send the message to ChatGPT API and returns its answer\n",
    "        :prompt: is the user prompt\n",
    "        :model: (optional) indicates the GPT model\n",
    "        :return: returns the answer from ChatGPT\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "def getAllData(data_dirpath):\n",
    "    \"\"\"\n",
    "    getAllData loads all data using readFunctions\n",
    "        :data_dirpath: Directory path of the file that contains all files to load\n",
    "        :return: returns all data in a document with .load() format\n",
    "    \"\"\"\n",
    "    # Read PDFs\n",
    "    pdf_loader = DirectoryLoader(data_dirpath, glob=\"**/*.pdf\")\n",
    "        \n",
    "    # Read web URLs in .txt\n",
    "    with open(data_dirpath + \"/\" + \"webURLs.txt\") as f:\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        \n",
    "    webpages_loader = SeleniumURLLoader(urls=lines)\n",
    "    \n",
    "    loaders = [pdf_loader, webpages_loader]\n",
    "    documents = []\n",
    "    \n",
    "    for loader in loaders:\n",
    "        documents.extend(loader.load())\n",
    "                    \n",
    "    return documents            \n",
    "    \n",
    "def getChunkText(documents):\n",
    "    \"\"\"\n",
    "    getChunkText function chunks all text data in chunks\n",
    "        :text: text data\n",
    "        :return: chunks of data\n",
    "    \"\"\"\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "    separator = \".\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200,\n",
    "    length_function = len\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n",
    "def chatWithDocs(question, chat_history):\n",
    "    response = chat_with_docs({\"question\": question,\n",
    "                           \"chat_history\": chat_history})\n",
    "    return response[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = getAllData(\"data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### documents in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = getChunkText(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chunks in ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunks[22])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Langchain - Conversation Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "chat_with_docs = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatWithDocs(\"Hi, I would like to do some exercise. I want to gain legs muscles\", chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatWithDocs(\"que es la maquina de turing\", chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POST request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=['*'],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=['*'],\n",
    "    allow_headers=['*'],\n",
    ")\n",
    "\n",
    "class Prompt(BaseModel):\n",
    "    user_prompt : str\n",
    "\n",
    "@app.post('/chat_with_docs')\n",
    "async def Post_prompt(prompt : Prompt):\n",
    "    return {\"response\" : chatWithDocs(prompt.user_prompt, chat_history)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrok_tunnel = ngrok.connect(8000)\n",
    "print('Public URL:', ngrok_tunnel.public_url)\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app, port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
